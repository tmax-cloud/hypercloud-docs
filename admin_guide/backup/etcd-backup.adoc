= etcd 백업 및 복구
:toc:
:toc-title:

본 장에서는 etcd와 통신하기 위한 cli인 etcdctl 설치 및 alias 설정과 etcd 백업 및 복구 과정에 대해 설명한다.

== 개요

etcd는 모든 클러스터 데이터(nodes, pods, configs, secrets, accounts, roles, etc.)에 대한 key-value 형태의 Kubernetes의 백업 저장소(기본 데이터 저장소)이다.


== 사전 작업

=== etcdctl 설치
. *설치*
+
기존에 etcdctl이 설치되지 않은 경우 etcdctl을 설치한다. 각 버전에 따른 설치 방법은 link:https://github.com/etcd-io/etcd/releases[]를 참고한다.
+
.3.5.7 버전 설치 예시
----
$ ETCD_VER=v3.5.7

# choose either URL
$ GOOGLE_URL=https://storage.googleapis.com/etcd
$ GITHUB_URL=https://github.com/etcd-io/etcd/releases/download
$ DOWNLOAD_URL=${GOOGLE_URL}

$ wget ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz
$ tar xzvf etcd-${ETCD_VER}-linux-amd64.tar.gz
$ mv etcd-${ETCD_VER}-linux-amd64/etcdctl /usr/local/bin/etcdctl
$ rm -f etcd-${ETCD_VER}-linux-amd64.tar.gz
----

. *설치 확인*
+
etcdctl가 정상적으로 설치되었는지 확인한다.
+
----
$ etcdctl version
----

=== etcdctl 명령어 사용

TLS 인증서 기반으로 etcd를 설치한 경우 etcdctl을 사용하려면 보안 통신을 위해 인증서 정보를 명령어 내 플래그로 넘겨주어야 한다. +
etcd가 구동 중인 서버가 아닌 다른 서버에서 etcd 백업을 위해서는 etcd 서버에 있는 인증 정보(server.crt, server.key, ca.crt)를 복사해 와야 한다. +
명령어를 통해 인증 과정을 거치지 않을 경우 etcdctl 명령어 수행 시 "context deadline exceed" 에러가 발생한다.

다음은 보안 통신을 사용하는 etcdctl 명령의 사용 예이다.

.예시
----
$ ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
<etcdctl 명령어>
----
각 노드마다 entpoint와 인증서 파일 이름을 확인 후 변경한다.
[width="100%",options="header", cols="1,2"]
|====================
|항목|설명
|ETCDCTL_API|etcdctl API 버전을 지정한다. 해당 환경 변수를 따로 지정하지 않을 경우 etcdctl API 버전 2를 사용하므로 3 버전의 API를 사용하기 위해 ETCDCTL_API=3을 선언한다.
|--cert, key|k8s에서 etcd에 접근할 수 있는 유일한 component인 kube-apiserver에서 사용하는 client 인증서와 key를 지정한다.
|--cacert|etcd 설정 시 생성한 certificate authority 파일을 지정한다.
|====================

=== etcdctl alias 설정
etcdctl 명령어를 사용하기 쉽게 alias로 등록한다. etcdctl을 사용할 각 노드마다 entpoint와 인증서 파일 이름을 확인 후 변경한다.
----
alias etcdctl='ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key'
----

[NOTE]
====
옵션 값인 certificate files 경로를 확인하는 방법은 두 가지 방식이 있다.

* *etcd 프로세스 조회*
+
현재 실행 중인 etcd 프로세스를 조회한다.
+
----
# --cert 옵션
$ ps -ef | grep etcd | grep cert-file

# --key 옵션
$ ps -ef | grep etcd | grep key-file

# --cacert 옵션
$ ps -ef | grep etcd | grep trusted-ca-file

# 결과
root     3943123 3943110  2 Feb17 ?        01:43:21 etcd --advertise-client-urls=https://172.22.7.3:2379 --auto-compaction-retention=8 --cert-file=/etc/kubernetes/ssl/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --election-timeout=5000 --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --heartbeat-interval=250 --initial-advertise-peer-urls=https://172.22.7.3:2380 --initial-cluster=master1=https://172.22.7.3:2380 --key-file=/etc/kubernetes/ssl/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://172.22.7.3:2379 --listen-metrics-urls=http://127.0.0.1:2381,http://172.22.7.3:2381 --listen-peer-urls=https://172.22.7.3:2380 --metrics=basic --name=master1 --peer-cert-file=/etc/kubernetes/ssl/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/ssl/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/ssl/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/ssl/etcd/ca.crt
----
* *etcd.yaml 파일 조회*
+
``/etc/kubernetes/manifests/etcd.yaml``의 **spec.containers.command**를 확인한다.
+
----
spec:
  containers:
    - command:
      ...
      - --cert-file=/etc/kubernetes/ssl/etcd/server.crt
      ...
      - --key-file=/etc/kubernetes/ssl/etcd/server.key
      ...
      - --trusted-ca-file=/etc/kubernetes/ssl/etcd/ca.crt
      ...
----
====

== 백업

etcd 클러스터 데이터를 주기적으로 백업하는 것을 권장한다. 스냅샷을 생성해도 etcd 멤버의 성능에 영향을 미치지 않는다. + 
클러스터 복원 시 모든 멤버는 **동일한 스냅샷을 사용하여 복원**해야 하므로, 복원을 위해 스냅샷을 생성할 경우 하나의 스냅샷 DB 파일만 있으면 된다.

=== etcdctl snapshot save 명령어를 사용하여 백업
현재 etcd 프로세스를 사용하는 활성 멤버에서 etcd API를 사용하여 스냅샷을 생성한다. +
해당 방법을 통해 만들어진 DB 파일은 무결성 해시(hash)를 포함하고 있어 추후 `etcdctl snapshot restore` 명령으로 복구할 때 파일이 변조되었는지 무결성을 선택적으로 확인 가능하다.

. *스냅샷 생성*
+
`etcdctl snapshot save` 명령을 사용하여 스냅샷을 생성한다.
+
.실행 방법
----
$ etcdctl snapshot save [백업 파일 이름(스냅샷 파일 경로 포함)]
----
+
.예시
----
# etcdctl snapshot save snapshot.db

{"level":"info","ts":"2023-02-20T16:20:23.698+0900","caller":"snapshot/v3_snapshot.go:65","msg":"created temporary db file","path":"/root/snapshot.db.part"}
{"level":"info","ts":"2023-02-20T16:20:23.711+0900","logger":"client","caller":"v3@v3.5.7/maintenance.go:212","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":"2023-02-20T16:20:23.711+0900","caller":"snapshot/v3_snapshot.go:73","msg":"fetching snapshot","endpoint":"https://127.0.0.1:2379"}
{"level":"info","ts":"2023-02-20T16:20:47.269+0900","logger":"client","caller":"v3@v3.5.7/maintenance.go:220","msg":"completed snapshot read; closing"}
{"level":"info","ts":"2023-02-20T16:20:47.426+0900","caller":"snapshot/v3_snapshot.go:88","msg":"fetched snapshot","endpoint":"https://127.0.0.1:2379","size":"643 MB","took":"23 seconds ago"}
{"level":"info","ts":"2023-02-20T16:20:47.426+0900","caller":"snapshot/v3_snapshot.go:97","msg":"saved","path":"/root/snapshot.db"}
Snapshot saved at /root/snapshot.db
----
+
NOTE: 백업 파일 이름을 ``etcd-date +%Y%m%d_%H%M%S``로 설정하면 'etcd-20230220_160848'와 같이 생성 날짜와 시간으로 파일 이름이 생성된다.

. *생성한 스냅샷 확인*
+
스냅샷 파일 경로에서 파일 목록을 조회하여 스냅샷 파일이 생성되어 있는지 확인한다.
+
----
$ ls -al
-rw------- 1 root   root  643133472 Feb 20 16:20 snapshot.db
----
+
이후 조회된 스냅샷 파일의 상태를 확인한다.
+
.실행 방법
----
$ etcdctl snapshot status [백업 파일 이름(스냅샷 파일 경로 포함)] --write-out=table
----
+
.예시
----
# etcdctl snapshot status snapshot.db --write-out=table

+----------+------------+------------+------------+
|   HASH   |  REVISION  | TOTAL KEYS | TOTAL SIZE |
+----------+------------+------------+------------+
| 21efa0a0 | 1179686689 |      18946 |     643 MB |
+----------+------------+------------+------------+
----

=== DB 파일 백업
etcd 데이터 파일 경로(data dir)에 존재하는 DB 파일을 복사하여 DB 파일을 백업한다. 현재 etcd 프로세스를 사용하는 활성 멤버가 없을 경우 사용한다. +
해당 방법을 통해 만들어진 DB 파일은 무결성 해시(hash)를 포함하고 있지 않아 추후 `etcdctl snapshot restore` 명령으로 복구할 때 `--skip-hash-check` 옵션을 추가하여 복구를 진행해야 하므로, `etcdctl snapshot save` 명령을 사용하여 스냅샷을 생성하는 것을 권장한다.

. *member/snap/db 파일 복사*
+
.실행 방법
----
$ cp [etcd 데이터 파일 경로]/member/snap/db [백업 파일 이름(스냅샷 파일 경로 포함)]
----
+
.예시
----
# cp /var/lib/etcd/member/snap/db ~/snapshot.db
----
+
[NOTE]
====
etcd의 데이터 파일 경로를 확인하는 방법은 두 가지 방식이 있다.

* *etcd 프로세스 조회*
+
현재 실행 중인 etcd 프로세스를 조회한다.
+
----
$ ps -ef | grep etcd | grep data-dir

# 결과
root     17716 17703 14 Jan25 ?        3-22:03:52 etcd --advertise-client-urls=https://172.21.4.2:2379 --auto-compaction-retention=8 --cert-file=/etc/kubernetes/ssl/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --election-timeout=5000 --heartbeat-interval=250 --initial-advertise-peer-urls=https://172.21.4.2:2380 --initial-cluster=master1=https://172.21.4.2:2380 --key-file=/etc/kubernetes/ssl/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://172.21.4.2:2379 --listen-metrics-urls=http://127.0.0.1:2381,http://172.21.4.2:2381 --listen-peer-urls=https://172.21.4.2:2380 --metrics=basic --name=master1 --peer-cert-file=/etc/kubernetes/ssl/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/ssl/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/ssl/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/ssl/etcd/ca.crt
----
* *etcd.yaml 파일 조회*
+
``/etc/kubernetes/manifests/etcd.yaml``의 **spec.containers.command**를 확인한다.
+
----
spec:
  containers:
    - command:
        ...
        - --data-dir=/var/lib/etcd
----
====

. *생성한 DB 백업 파일 확인*
+
DB 파일을 복사한 경로에서 파일 목록을 조회하여 확인한다.
+
----
$ ls -al
-rw------- 1 root   root  643133472 Feb 20 16:20 snapshot.db
----

== 복구

etcd 복구란 스냅샷을 생성했던 시점으로 다시 되돌리겠다는 것을 의미한다. +
클러스터를 복구하려면 하나의 etcd 스냅샷 DB 파일이 필요하며 **모든 멤버는 동일한 스냅샷을 사용**하여 복원해야 한다.

etcd 복구는 "백업한 DB 파일을 사용하여 복구" → "etcd yaml 파일 수정" → "복구 확인" 순으로 진행한다. *단, data-dir를 변경하지 않는 경우 "etcd yaml 파일 수정" 과정은 생략한다.*

=== 사전 작업
==== 단일 control plain 구성 (Master 1대)
기존 etcd 데이터 파일 경로를 삭제한다.
----
$ mv /var/lib/etcd /var/lib/etcd-old
$ mkdir /var/lib/etcd
----

==== 다중 control plain 구성 (Master 2대 이상)

. *모든 etcd 노드에 스냅샷 복사*
+
모든 etcd 노드는 **동일한 스냅샷을 사용**하여 복구해야 하므로 모든 etcd 노드에 스냅샷을 복사한다. +
+
.예시
----
$ scp snapshot.db root@172.21.4.3:/root
----

. *모든 etcd 노드에서 기존 etcd 데이터 파일 경로 삭제*
+
----
$ mv /var/lib/etcd /var/lib/etcd-old
$ mkdir /var/lib/etcd
----
    
=== 백업한 DB 파일을 사용하여 복구

==== 단일 control plain 구성 (Master 1대)
.실행 방법
----
$ etcdctl snapshot restore [백업 파일 이름(스냅샷 파일 경로 포함)] \
--name=[호스트 이름] \
--data-dir=/var/lib/etcd \
--initial-cluster=[호스트 이름]=https://[호스트 IP 주소]:2380 \
--initial-advertise-peer-urls=https://[호스트 IP 주소]:2380 \
----

.예시
----
$ etcdctl snapshot restore snapshot.db \
--name=master1 \
--data-dir=/var/lib/etcd \
--initial-cluster=master1=https://172.21.4.1:2380 \
--initial-advertise-peer-urls=https://172.21.4.1:2380 \
----
* initial-cluster 값으로 member 정보를 덮어쓴다.
* 테스트 등의 목적으로 여러 개의 클러스터를 가동하는 경우 클러스터를 구분하기 위해 각 클러스터에 고유한 값인 `--initial-cluster-token=<etcd-cluster-name>` 옵션을 사용해야 한다.
* `etcdctl snapshot save` 명령을 통해 스냅샷을 생성한 것이 아니라, DB 파일을 복사하여 백업한 파일을 통해 복구를 진행할 경우 무결성 해시(hash)를 포함하고 있지 않아 `--skip-hash-check` 옵션을 추가해야 한다.

==== 다중 control plain 구성 (Master 2대 이상)
etcd cluster로 구성한 경우 모든 etcd node에서 동일한 작업을 수행해야 한다.

.실행 방법
----
$ etcdctl snapshot restore [백업 파일 이름(스냅샷 파일 경로 포함)] \
--name [호스트1 이름] \
--data-dir [etcd 데이터 파일 경로] \
--initial-cluster [호스트1 이름]=https://[호스트1 IP 주소]:2380,[호스트2 이름]=https://[호스트2 IP 주소]:2380,[호스트3 이름]=https://[호스트3 IP 주소]:2380 \
--initial-advertise-peer-urls https://[호스트1 IP 주소]:2380

$ etcdctl snapshot restore [백업 파일 이름(스냅샷 파일 경로 포함)] \
--name [호스트2 이름] \
--data-dir [etcd 데이터 파일 경로] \
--initial-cluster [호스트1 이름]=https://[호스트1 IP 주소]:2380,[호스트2 이름]=https://[호스트2 IP 주소]:2380,[호스트3 이름]=https://[호스트3 IP 주소]:2380 \
--initial-advertise-peer-urls https://[호스트2 IP 주소]:2380

$ etcdctl snapshot restore [백업 파일 이름(스냅샷 파일 경로 포함)] \
--name [호스트3 이름] \
--data-dir [etcd 데이터 파일 경로] \
--initial-cluster [호스트1 이름]=https://[호스트1 IP 주소]:2380,[호스트2 이름]=https://[호스트2 IP 주소]:2380,[호스트3 이름]=https://[호스트3 IP 주소]:2380 \
--initial-advertise-peer-urls https://[호스트3 IP 주소]:2380
----
.예시
----
* master1 복구
$ etcdctl snapshot restore snapshot.db \
--name master1 \
--data-dir /var/lib/etcd \
--initial-cluster master1=https://172.21.4.2:2380,master2=https://172.21.4.3:2380,master3=https://172.21.4.4:2380 \
--initial-advertise-peer-urls=https://172.21.4.2:2380

* master2 복구
$ etcdctl snapshot restore snapshot.db \
--name master2 \
--data-dir /var/lib/etcd \
--initial-cluster master1=https://172.21.4.2:2380,master2=https://172.21.4.3:2380,master3=https://172.21.4.4:2380 \
--initial-advertise-peer-urls=https://172.21.4.3:2380

* master3 복구
$ etcdctl snapshot restore snapshot.db \
--name master3 \
--data-dir /var/lib/etcd \
--initial-cluster master1=https://172.21.4.2:2380,master2=https://172.21.4.3:2380,master3=https://172.21.4.4:2380 \
--initial-advertise-peer-urls=https://172.21.4.4:2380
----
* --data-dir : 백업된 파일을 복구한 후 생성되는 데이터 파일의 위치이다. 기존 data-dir가 아닌 다른 위치로 변경한 경우 복구 후 etcd yaml에서 data-dir 값을 변경해야 한다.
* initial-cluster 값으로 member 정보를 덮어쓴다.
* 테스트 등의 목적으로 여러 개의 클러스터를 가동하는 경우 클러스터를 구분하기 위해 각 클러스터에 고유한 값인 `--initial-cluster-token=<etcd-cluster-name>` 옵션을 사용해야 한다.
* `etcdctl snapshot save` 명령을 통해 스냅샷을 생성한 것이 아니라, DB 파일을 복사하여 백업한 파일을 통해 복구를 진행할 경우, 무결성 해시(hash)를 포함하고 있지 않아 `--skip-hash-check` 옵션을 추가해야 한다.

[NOTE]
====
옵션 값을 확인하는 방법은 두 가지 방식이 있다.

* *etcd 프로세스 조회*
+
현재 실행 중인 etcd 프로세스를 조회한다.
+
----
$ ps -ef | grep etcd

# 결과
root     17716 17703 14 Jan25 ?        3-17:53:25 etcd --advertise-client-urls=https://172.21.4.2:2379 --auto-compaction-retention=8 --cert-file=/etc/kubernetes/ssl/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --election-timeout=5000 --heartbeat-interval=250 --initial-advertise-peer-urls=https://172.21.4.2:2380 --initial-cluster=master1=https://172.21.4.2:2380 --key-file=/etc/kubernetes/ssl/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://172.21.4.2:2379 --listen-metrics-urls=http://127.0.0.1:2381,http://172.21.4.2:2381 --listen-peer-urls=https://172.21.4.2:2380 --metrics=basic --name=master1 --peer-cert-file=/etc/kubernetes/ssl/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/ssl/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/ssl/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/ssl/etcd/ca.crt
----
* *etcd.yaml 파일 조회*
+
``/etc/kubernetes/manifests/etcd.yaml``의 **spec.containers.command**를 확인한다.
+
----
spec:
  containers:
    - command:
        ...
        - --data-dir=/var/lib/etcd
        - --initial-advertise-peer-urls=https://172.21.4.2:2380
        - --initial-cluster=master1=https://172.21.4.2:2380
        ...
        - --name=hc5-master1
----
====


=== etcd yaml 파일 수정

etcd pod를 생성하는 yaml 파일을 수정하여 etcd의 데이터 파일 디렉터리를 변경한다. 다중 control plain 구성(Master 2대 이상)일 경우에는 모든 노드에서 변경한다. +
yaml 파일을 변경하여 저장하면 etcd pod가 static pod인 관계로 etcd pod를 자동으로 다시 재생성하게 되며, 이때 데이터 파일이 있는 디렉터리는 변경한 디렉터리를 바라보게 된다.

.예시
----
$ vim /etc/kubernetes/manifests/etcd.yaml

# 예시 (etcdctl snapshot restore 명령에서 --data-dir /var/lib/etcd-backup으로 수행한 경우)
// 기존 spec.containers.command의 data-dir
spec:
  containers:
  - command:
    ...
    - --data-dir=/var/lib/etcd

// 아래와 같이 변경
spec:
  containers:
  - command:
    ...
    - --data-dir=/var/lib/etcd-backup
----

=== 복구 확인
. 기존의 etcd pod가 죽고 다시 생성되는지 확인한다.

. etcd member list를 조회한다.
+
----
$ etcdctl member list -w table
----

. kube-system이 정상화되지 않을 경우 kubelet 재시작 및 etcd와 k8s-api-server log를 확인한다.
